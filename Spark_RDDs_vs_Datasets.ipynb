{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWn6AC170NfVCG8quyu1jO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoPrieto/ELIA/blob/master/Spark_RDDs_vs_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diferencias entre RDDs y Datasets en Apache Spark\n",
        "\n",
        "En Apache Spark, los Resilient Distributed Datasets (RDDs) y los Datasets son dos abstracciones de datos distribuidas que permiten realizar operaciones paralelas en grandes conjuntos de datos. A continuación, se presentan algunas diferencias clave:\n",
        "\n",
        "## 1. Tipos de Datos\n",
        "\n",
        "* ### RDDs (Resilient Distributed Datasets)\n",
        "\n",
        "Los RDDs son una colección distribuida de objetos inmutables y tolerantes a fallos. Pueden contener cualquier tipo de objeto y se almacenan en particiones distribuidas a través de un clúster Spark. Sin embargo, la información sobre el tipo de datos se pierde cuando se almacenan en un RDD, y las operaciones en RDDs son principalmente de bajo nivel.\n",
        "\n",
        "* ### Datasets\n",
        "\n",
        "Los Datasets son una extensión de los RDDs y proporcionan una API más rica y orientada a tipos de datos más fuertes. Están diseñados para trabajar con datos estructurados mediante la introducción de un sistema de tipos de datos más rico que los RDDs. Pueden trabajar con datos de tipo primitivo y también con objetos de tipo usuario, manteniendo la información sobre el tipo de datos.\n",
        "\n",
        "## 2. Optimización de Consultas\n",
        "\n",
        "* ### RDDs\n",
        "\n",
        "Las operaciones en RDDs son evaluadas de manera perezosa (lazy evaluation), lo que significa que las transformaciones no se ejecutan de inmediato, sino que se planifican para su ejecución cuando se dispara una acción. Sin embargo, el motor de Spark no tiene un conocimiento profundo de la estructura de datos dentro de un RDD, lo que puede limitar las oportunidades de optimización.\n",
        "\n",
        "* ### Datasets\n",
        "\n",
        "Los Datasets, al ser orientados a tipos, permiten un mejor rendimiento a través de la optimización de consultas y el uso de la reflexión de tipos para aplicar reglas de optimización durante la compilación. Esto puede resultar en un rendimiento más eficiente en comparación con RDDs para operaciones complejas.\n",
        "\n",
        "## 3. Tolerancia a Fallos\n",
        "\n",
        "* ### RDDs\n",
        "\n",
        "Los RDDs son inmutables y tolerantes a fallos por diseño. Cualquier transformación en un RDD crea un nuevo RDD en lugar de modificar el RDD existente. Esto facilita la recuperación ante fallos.\n",
        "\n",
        "* ### Datasets\n",
        "\n",
        "Los Datasets también son tolerantes a fallos y ofrecen una tolerancia a fallos similar a los RDDs.\n",
        "\n"
      ],
      "metadata": {
        "id": "nfCw3VLfuEOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDDs"
      ],
      "metadata": {
        "id": "qtRXYrPD_udg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Los RDDs usan la programación orientada a objetos como forma de orquestar los ETL\n",
        "importante tener una buena comprensión de lambda puesto que es crucial dentro de esta tecnologia\n",
        "\"\"\"\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"EjemploRDD\")\n",
        "\n",
        "datos_temperatura = [(\"CiudadA\", 25.5), (\"CiudadB\", 30.0), (\"CiudadA\", 28.3),\n",
        "                     (\"CiudadB\", 32.1), (\"CiudadA\", 26.8), (\"CiudadB\", 31.5)]\n",
        "\n",
        "rdd_temperatura = sc.parallelize(datos_temperatura)\n",
        "\n",
        "rdd_promedio_por_ciudad = (\n",
        "    rdd_temperatura\n",
        "    .map(lambda x: (x[0], (x[1], 1)))\n",
        "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
        "    .mapValues(lambda x: x[0] / x[1])\n",
        ")\n",
        "\n",
        "\n",
        "resultados = rdd_promedio_por_ciudad.collect()\n",
        "for ciudad, promedio in resultados:\n",
        "    print(f\"La temperatura promedio en {ciudad} es {promedio} grados Celsius.\")\n",
        "\n",
        "# Importante para el contexto que se creo, puesto que podria haber problemas mas adelante\n",
        "\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "H3VPb2co0eAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Datasets*\n",
        "\n",
        "Este enfoque con DataFrames es más conciso y fácil de leer en comparación con el uso directo de RDDs. Además, los DataFrames ofrecen optimizaciones internas, como la ejecución distribuida y la optimización de consultas, que pueden mejorar el rendimiento en comparación con RDDs para muchas operaciones comunes."
      ],
      "metadata": {
        "id": "J6Yon7R6_6gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EjemploDataset\").getOrCreate()\n",
        "\n",
        "## Retomamos el ejemplo anterior\n",
        "datos_temperatura = [(\"CiudadA\", 25.5), (\"CiudadB\", 30.0), (\"CiudadA\", 28.3),\n",
        "                     (\"CiudadB\", 32.1), (\"CiudadA\", 26.8), (\"CiudadB\", 31.5)]\n",
        "\n",
        "columnas = [\"Ciudad\", \"Temperatura\"]\n",
        "df_temperatura = spark.createDataFrame(datos_temperatura, columns)\n",
        "\n",
        "# Realizar operaciones con el DataFrame para calcular el promedio de temperatura por ciudad\n",
        "# Para esto podemos manejarlo de la misma forma que pandas\n",
        "df_promedio_por_ciudad = (\n",
        "    df_temperatura\n",
        "    .groupBy(\"Ciudad\")\n",
        "    .agg(F.avg(\"Temperatura\").alias(\"PromedioTemperatura\"))\n",
        ")\n",
        "\n",
        "\n",
        "df_promedio_por_ciudad.show()\n",
        "\n",
        "# Detener la SparkSession\n",
        "\"\"\"IMPORTANTE\"\"\"\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "GUOwUt22_rpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets usando lenguaje SQL\n",
        "\n",
        "es posible utilizar lenguaje SQL en PySpark con DataFrames. PySpark proporciona una interfaz SQL que te permite ejecutar consultas SQL directamente sobre los DataFrames.\n",
        "Este enfoque es útil cuando estás familiarizado con SQL y prefieres expresar tus operaciones de manipulación de datos de esa manera. Ten en cuenta que, internamente, Spark optimizará la ejecución de estas consultas SQL para aprovechar al máximo su motor distribuido."
      ],
      "metadata": {
        "id": "Z8Dy8mM4CaHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EjemploSQL\").getOrCreate()\n",
        "\n",
        "datos_temperatura = [(\"CiudadA\", 25.5), (\"CiudadB\", 30.0), (\"CiudadA\", 28.3),\n",
        "                     (\"CiudadB\", 32.1), (\"CiudadA\", 26.8), (\"CiudadB\", 31.5)]\n",
        "\n",
        "columnas = [\"Ciudad\", \"Temperatura\"]\n",
        "df_temperatura = spark.createDataFrame(datos_temperatura, columnas)\n",
        "\n",
        "# Registrar el DataFrame como una tabla temporal\n",
        "df_temperatura.createOrReplaceTempView(\"tabla_temperatura\")\n",
        "\n",
        "# Ejecutar una consulta SQL sobre el DataFrame\n",
        "resultado_sql = spark.sql(\"SELECT Ciudad, AVG(Temperatura) as PromedioTemperatura FROM tabla_temperatura GROUP BY Ciudad\")\n",
        "\n",
        "\n",
        "resultado_sql.show()\n",
        "\n",
        "# Detener la SparkSession\n",
        "\"\"\"IMPORTANTE\"\"\"\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "4BgSoaDGCFdu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}